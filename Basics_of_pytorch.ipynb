{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "option ->  (training, test), necessary for load_mnist(option, path) function.\n",
    "\"\"\"\n",
    "\n",
    "class DatasetFashion():\n",
    "    \n",
    "    def __init__(self, option, classes = range(10), crossvalidation=None, path='./'):\n",
    "        X,y = load_mnist(option, path)       #loading function which provides the dataset in suitable format\n",
    "\n",
    "        self.images = X.reshape([-1, 28*28])  #dimensions are arranged (All pixels have to be in columns)\n",
    "        self.images = self.images/255.0       #normalization of pixel values to [0,1] range\n",
    "        self.labels = y.type(th.long) \n",
    "\n",
    "        #eliminating unwanted classes and sorting at the same time\n",
    "        indices = th.cat([th.nonzero(self.labels == i) for i in classes], dim=0)\n",
    "        self.images = self.images[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "        \n",
    "        #shuffling\n",
    "        indices = th.randperm(len(self.labels))\n",
    "        self.images = self.images[indices]\n",
    "        self.labels = self.labels[indices]\n",
    "          \n",
    "        if crossvalidation:\n",
    "            splitSize = len(self.labels)//crossvalidation[0]\n",
    "            chunks = th.split(th.arange(len(self.labels)), splitSize, dim=0)\n",
    "            validation = chunks[crossvalidation[1]]\n",
    "            training = th.cat([c for j, c in enumerate(chunks) if j!=crossvalidation[1]], dim=0)\n",
    "            self.validationImages = self.images[validation]\n",
    "            self.validationLabels = self.labels[validation]\n",
    "            self.trainingImages = self.images[training]\n",
    "            self.trainingLabels = self.labels[training]\n",
    "\n",
    "        else:    \n",
    "            split_ratio = int(len(self.images)*4/5)\n",
    "            \n",
    "            #split the data\n",
    "            self.trainingImages = self.images[:split_ratio]\n",
    "            self.trainingLabels = self.labels[:split_ratio]\n",
    "            self.validationImages = self.images[split_ratio:]\n",
    "            self.validationLabels = self.labels[split_ratio:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trainingLabels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.trainingImages[index], self.trainingLabels[index]\n",
    "            \n",
    "class Network(th.nn.Module):\n",
    "    def __init__(self, num_inputs, num_classes, num_neurons=(50,20), activation=th.relu):\n",
    "        super(Network, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.logsoftmax = th.nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.layers = th.nn.ModuleList()\n",
    "        self.layers.append(th.nn.Linear(num_inputs, num_neurons[0], True))\n",
    "        for i in range(len(num_neurons)-1):\n",
    "            self.layers.append(th.nn.Linear(num_neurons[i], num_neurons[i+1], True))\n",
    "        self.layers.append(th.nn.Linear(num_neurons[-1], num_classes, True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        return self.logsoftmax(self.layers[-1](x))\n",
    "    \n",
    "def train(model, dataloader, optimizer):\n",
    "    \n",
    "    loss_hist = np.array([])\n",
    "    for index, (trainingImages,trainingLabels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(trainingImages.squeeze())\n",
    "        loss = th.nn.functional.nll_loss(prediction, trainingLabels.squeeze())\n",
    "        loss_hist = np.append(loss_hist, loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with th.no_grad():\n",
    "            if index % 100 == 0:\n",
    "                print('Train {}/{} Loss {:.6f}'.format(index, len(dataloader), loss.item()))\n",
    "    return loss_hist\n",
    "                \n",
    "def evaluate(model, images, labels):\n",
    "    prediction = model(images.squeeze())\n",
    "    loss = th.nn.functional.nll_loss(prediction, labels.squeeze())\n",
    "    pred_label = th.argmax(prediction, dim=1)\n",
    "    #print(\"----------\", pred_label[:10], labels.squeeze()[:10], pred_label[:10] == labels.squeeze()[:10])\n",
    "    accuracy = ((pred_label == labels.squeeze()).sum().item()) / len(images)\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = Network(28*28, 10)\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    " \n",
    "    training_loss_hist = np.array([])   #also validation loss and accuracy history can be kept\n",
    "        \n",
    "    dataset = DatasetFashion(\"training\") \n",
    "    dataloader = th.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        print(\"Epoch \", epoch)\n",
    "        #training\n",
    "        loss_hist_per_epoch = train(model, dataloader, optimizer)\n",
    "\n",
    "        #loss and accuracy histories are kept\n",
    "        training_loss_hist = np.hstack((training_loss_hist, loss_hist_per_epoch))\n",
    "        \n",
    "    #evaluation\n",
    "    valid_loss, valid_accuracy = evaluate(model, dataset.validationImages, dataset.validationLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = Network(28*28, 10, activation = th.sigmoid)\n",
    "    optimizer = th.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    training_loss_hist = np.array([])   #also validation loss and accuracy history can be kept\n",
    "    avg_loss = 0\n",
    "    for i in range(5):\n",
    "        \n",
    "        dataset = DatasetFashion(\"training\", crossvalidation=(5,i))\n",
    "        dataloader = th.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "        \n",
    "        for epoch in range(10):\n",
    "            #training\n",
    "            loss_hist_per_epoch = train(model, dataloader, optimizer)\n",
    "\n",
    "            #loss and accuracy histories are kept\n",
    "            training_loss_hist = np.hstack((training_loss_hist, loss_hist_per_epoch))\n",
    "            \n",
    "        #evaluation\n",
    "        valid_loss, valid_accuracy = evaluate(model, dataset.validationImages, dataset.validationLabels)\n",
    "            \n",
    "        avg_loss += evaluate(model, dataset.validationImages, dataset.validationLabels)/5\n",
    "        \n",
    "    print(\"Average Loss\", avg_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "311.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
